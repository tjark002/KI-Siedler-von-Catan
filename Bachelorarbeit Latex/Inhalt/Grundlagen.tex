\chapter{Grundlagen}
Da diese Ausarbeitung die Entwicklung eines Strategiespiels für computergesteuerte Spieler behandelt, sollen in diesem Kapitel die Grundlagen eines solchen Spiels erläutert werden. Dieses Kapitel ist insofern von besonderer Bedeutung, als dass der Entwurf und die darauf basierende Implementation des Spieles auf den hier dargestellten Grundlagen basieren. Ferner wird in der Evaluation des Spieles nochmals Bezug auf dieses genommen. \\

Die Entwicklung computergesteuerter Spieler reicht inzwischen schon über 20 Jahre zurück. So schrieben \cite[vgl. S. 24ff]{Davis1999} 1999 von ihren Entwicklungen, die beispielsweise in das bekannte Strategiespiel \textit{Civilization: Call To Power} Einzug fanden. Das ist u.a deshalb beeindruckend, weil der Sieg von Deep Blue über Garri Kasparow in Spiel Schach zu diesem Zeitpunkt erst 3 Jahre zurück lag. Seitdem hat sich sowohl für Echtzeitspiele als auch für rundenbasierte Spiele viel getan. Bis heute ist das einprogrammierte \quotes{Cheaten} der Computergegner ein Problem, was von \cite[vgl. S. 1]{Ruiz2007} näher beschrieben wurde, wobei ein dieses Problem im Kontext dieser Arbeit nicht auftreten wird, da das Ziel dieser Arbeit nicht ist, eine möglichst spannende KI zu erstellen. Die Arbeit von \cite{Ruiz2007} ist dennoch interessant, weil die Autoren sich speziell mit dem Case-Based-Reasoning auseinander gesetzt haben. Sie präsentieren zudem eine Idee für die Interaktion von Spiel und Spieler mittels einer \textit{Application Programming Interface} (API). Die API sei in der Lage, eine Verbindung zwischen der KI und dem Spiel zu schaffen, sodass die KI Zugriff auf alle Informationen habe und gleichzeitig Anweisungen erteilen könne. Diese API ist vonnöten, da eine Konvertierung der auf dem Bildschirm erscheinenden Informationen erfolgen muss, um diese für einen Computer wieder verarbeitbar zu machen. Diese entsprechende Architektur lässt sich, wie in Abb. \ref{fig:Architektur} zu sehen ist, oberflächlich darstellen. \\

\begin{figure}[h]
	\centering
	\includegraphics[height=9cm]{Bilder/Architektur.png}
	\caption{Architektur eines KI-Spiels (In Anlehnung an \cite[vgl. S. 4]{Ruiz2007})}
	\label{fig:Architektur}
\end{figure}

Abb. \ref{fig:Architektur} zeigt drei Komponenten, die über vier Pfeile miteinander verbunden sind. Die oberste Komponente repräsentiert die KI. Sie steht nicht etwa für den Spieler, der im Spiel zu sehen ist, sondern nur für die dahinter stehende Logik - So wie ein menschlicher Spieler auch nicht die Figur im Spiel ist, sondern diese Figur nur eine Repräsentation seiner selbst darstellt. Auf gleiche Weise funktioniert dieses Prinzip auch mit einem computergesteuertem Spieler. Er erhält Informationen und erteilt Anweisungen über eine geeignete Schnittstelle (hier: die API) und wird im Spiel ohne eine andere Verkörperung durch eine Spielfigur verbildlicht. \\

Die unterste Komponente stellt das Spiel dar. Zwischen der KI- und der Spielkomponente befindet sich die API-Komponente. Diese ist die erwähnte Schnittstelle und ermöglicht eine Kommunikation zwischen dem Spiel und den KI-Spielern. Das Spiel kann über die API Informationen an einen Spieler senden und dem Spieler wird so das Erhalten von Informationen ermöglicht. Äquivalent dazu funktioniert das Senden und Empfangen von Anweisungen.Diesmal sendet jedoch der Spieler die Anweisungen ab, während das Spiel diese empfängt und ggf. verarbeitet. So wird dem Spieler die Steuerung seiner Repräsentation im Spiel ermöglicht, unabhängig davon, ob es sich um eine Figur oder ein Reich oder ähnliches handelt. \\

\begin{figure}[h]
	\centering
	\includegraphics[height=9cm]{Bilder/Architektur-analog-mensch.png}
	\caption{Architektur menschlicher Spieler (eigene Darstellung)}
	\label{fig:Architektur_menschlich}
\end{figure}

Abbildung \ref{fig:Architektur_menschlich} zeigt eine analoge Darstellung der Komponenten für menschliche Spieler. Diesmal stellt die oberste Komponente nicht etwa den KI-Spieler dar, sondern einen menschlichen Spieler. Und auch die mittlere Komponente ist eine andere. Zuvor wurde die Mitte durch die API veranschaulicht. Jetzt sind stattdessen zwei Komponenten zu sehen: \textit{Bildschirm/Grafik} und \textit{Tastatur/Maus}. Nur die untere Komponente \textit{Spiel} ändert sich nicht. Diese Abbildung soll die Parallelen zwischen der Verwendung eines KI-Spielers und der eines menschlichen Spielers weiter verdeutlichen, denn ob ein Mensch oder eine Maschine spielt, ist im Prinzip das Gleiche. Wichtig ist nur die Umsetzung der Schnittstelle und da meist ein Mensch das Spiel spielt, erscheinen Maus, Tastatur und Bildschirm als Schnittstelle intuitiver. Doch die Aufnahme von Informationen über einen Bildschirm und das Erteilen von Anweisungen mit Maus und Tastatur ist nichts anderes als das Verwenden einer Schnittstelle zum Steuern der eigenen Repräsentation im Spiel. Für die KI wird das Gleiche erreicht, wenn sie Informationen und Anweisungen über eine API erhält bzw. versendet. \\

Das beschriebene Prinzip gilt also nicht exklusiv für computergesteuerte Gegner. Anhand von Abb. \ref{fig:Architektur_menschlich} wird deutlich, dass es sich viel allgemeiner um einen Standardweg der Kommunikation zwischen einem Programm und einer nicht darin enthaltenen Entität handelt. \\

\section{Computergegner in rundenbasierten Strategiespielen}
Nachdem im obigen Abschnitt allgemein auf die Interaktion zwischen Spielern und dem Spiel eingegangen wurde, sollen im Folgenden ein möglicher Aufbau und die Konzeption von Computergegnern beschrieben werden. \\

Ein KI-Spieler kann auf unterschiedliche Weisen konzipiert werden. So ist es möglich, feste Strategien von Hand zu implementieren und diese den Spieler ausführen zu lassen. Des Weiteren können KIs durch Neuronale Netze und dem damit verbundenem Deep Learning realisiert werden \cite[vgl. S. 1]{Mayer2007}. AAußerdem ist es möglich, KI-Spieler zu entwickeln, die ihre Strategien mithilfe Wissensbasierter Systeme (WBS) umsetzen. \cite[vgl. S.5]{Ruiz2007}. Alle diese Varianten können gute Ergebnisse erzielen und ihre Einsatz hängt maßgeblich vom gewünschten Ziel ab: so würde eine von Hand programmierte KI dann zum Einsatz kommen, wenn nicht unbedingt die bestmögliche Performance gewünscht ist, aber stattdessen ein ganz bestimmtes Verhalten erzeugt werden soll. Ein Ziel, das beispielsweise durch Neuronale Netze nur schwer zu erreichen wäre. Wenn andererseits die stärkste mögliche KI als Ergebnis herauskommen soll, dann eignen sich Herangehensweisen, die die KIs ihre eigenen Methoden finden lassen und durch Lernprozesse auf diesem Wege die besten Strategien ausmachen kann. Dies ist etwas, was bei hart-kodierten Spielern wiederum äußert schwer, wenn überhaupt zu erreichen ist. \\

\subsection{Der KI-Spieler als Wissensbasiertes System}
\label{sec:WBS}
Da das Produkt dieser Arbeit jedoch hauptsächlich für den Einsatz von KI-Spielern nach dem Prinzip der WBS gebraucht werden soll, wird der Fokus der Erläuterungen auf diesen liegen. Abbildung \ref{fig:WBS} zeigt, wie ein WBS grundlegend aufgebaut ist. Es wird deutlich, dass eine Abgrenzung zwischen dem Interpreter und dem Wissen gezogen wird. Das ist einer der identifizierenden Punkte eines solchen Systems, da die Handlungen des Systems sind nicht nur vom Interpreter an sich abhängig sondern auch von der dahinter liegenden Wissensbasis. \\
Das Konzept des Systems lässt sich auch anhand der Abbildung erkennen. Zunächst erfolgt eine Eingabe, die dieser Interpreter wahrnimmt. Anschließend greift dieser zur Verarbeitung der Eingabe auf die angeschlossene Wissensbasis zurück und versucht, mit Hilfe dieser eine passende Lösung auf die Problemstellung zu finden. Schlussendlich wird die Lösung als Ausgabe zurückgeliefert. Bezogen auf eine KI im Kontext eines Videospiels sind die Eingaben als Informationen, die das Spiel liefert, zu verstehen. Man denke hierbei an Abb. \ref{fig:Architektur} zurück, bei der die KI ihre Informationen über die API erhalten hat. Diese Informationen sind demnach die Eingabe für das System. In der gleichen Abbildung wurde auch gezeigt, dass es sich bei der Ausgabe der KI um Anweisungen handelt. Demnach gibt auch das WBS in diesem Fall Anweisungen als Ausgabe zurück. Also muss durch den Interpreter eine Art Konvertierung einer Information in eine Handlung erfolgen, die von der Repräsentation des Spielers im Spiel ausgeführt werden kann. Hierbei ist dennoch zu beachten, dass nicht jede gesendete Information auch eine direkte Anweisung zur Folge haben muss. Die Informationen dienen vielmehr der Findung einer Strategie durch den Interpreter und der Interpreter, wobei dieser Aktionen der gefundenen Strategie auch dann ausführen lassen kann, wenn gerade gar kein Input erfolgt ist.\\
In Bezug auf die Fallbasis ist wichtig zu wissen, dass diese sowohl Wissen über das Spiel und die möglichen Strategien, als auch Wissen über das eigene System enthält. 

\begin{figure}[h]
	\centering
	\includegraphics[width=12cm]{Bilder/WissensbasiertesSystem.png}
	\caption{Wissensbasiertes System (Aus Vorlesung WBS von Herrn Althoff)}
	\label{fig:WBS}
\end{figure}

Eine mögliche Umsetzung eines WBS ist in Form einer fallbasierten Variante (auch Case Based Reasoning genannt). Bei dieser Herangehensweise wird die Wissensbasis mithilfe einer Fallbasis realisiert. Diese Fallbasis enthält - auf ein Videospiel bezogen - alle möglichen Zustände des Spiels, über die das System Bescheid weiß. Hieraus ergibt sich auch, dass eine \quotes{neue} KI über geringeres Wissen verfügt als eine trainierte, weil sie keine dazu Zeit hatte, sich genügend Fälle anzueignen. Was ein Fall enthält, muss definiert werden. Hiervon ist abhängig, wie gut die neuere KI nach dem Training spielen wird. \\

\cite[vgl. S. 106]{Weber2009} haben für das Spiel \textit{Wargus} eine eigene KI geschrieben, die mithilfe von Case Based Reasoning funktioniert. Ein Fall wurde bei ihnen durch mehrere Features definiert, die wiederum jeweils mehrere Eigenschaften abbildeten. So beschreibt jeder Fall den Forschungsstand des Gegners, den eigenen, die Anzahl an Kampfeinheiten (nach Typen aufgeteilt) und Arbeitern sowie die Menge an Produktionsgebäuden und Eigenschaften der Karte. Ein einzelner Fall enthält umfassende Informationen über den gesamten Spielstand, was durchaus nötig ist. Die Strategien werden aufgrund von Ähnlichkeiten mit schon bekannten Fällen gebildet. \\
Angenommen, ein Fall würde auf die Eigenschaften der Karte verzichten und nur die übrigen miteinbeziehen. Zusätzlich würden in der Fallbasis bereits zwei Fälle existieren und wird der ähnlichste dieser beiden zu einem neuen Fall gesucht. Sei nun außerdem angenommen, dass die Eigenschaften des neuen Fall - außer die der Karte - mit Fall 1 übereinstimmen und Fall 2 leicht davon abweicht, dafür hat Fall jedoch eine Karte, die sich nicht von der des neuen Falls unterscheidet. Abhängig davon, ob die Karte mit in die Bewertung der Ähnlichkeit einfließt, könnten nun zwei unterschiedliche ähnlichste Fälle gefunden werden und damit würde eine verschieden gute Strategie gebildet wird. Hieraus lässt sich ableiten, dass es zum Finden der bestmöglichen Strategie nötig ist, die Fälle so genau wie möglich zu beschreiben. \\
Dennoch muss laut \cite[vgl. S. 108]{Weber2009} ein Fall auch generalisiert werden. Sie wählten als einzige Information über den Gegner den Fortschritt seiner Forschung und bezeichneten dieses Symptom als wichtigsten Indikator für dessen Strategie - zumindest im betrachteten Spiel. Interessanterweise konkurrieren diesen beiden Annahmen nicht miteinander, was auf den ersten Blick jedoch paradox wirken kann. Einerseits sollen Fälle die aktuelle Situation so genau wie möglich beschreiben, andererseits wird sich jedoch zur Erkennung des Stands des Gegners auf ein einzelnes Merkmal bezogen. Ein solches Vorgehen ist dann sinnvoll, wenn die anderen Symptome deutlich schwerer zu erfassen wären oder nur unzureichend verlässliche Informationen liefern würden. Die Autoren beschrieben diesen Zustand als \textit{unverlässliche Informationsumgebung} \cite[vgl. S. 107f]{Weber2009}. Sie treffen die Annahme, dass es besser ist, sich auf ein Symptom zu verlassen, welches mit relativ hoher Wahrscheinlichkeit genau bestimmt werden kann, statt weitere, möglicherweise falsche, Merkmale hinzuzuziehen. Konkret bezeichnen sie die Bestimmung der Anzahl an Arbeitern und Truppen des Gegners als unzuverlässig, weil diese sich bewegen und ihre Anzahl nicht genau bestimmt werden kann. \\

Da zu jedem Fall einer Wissensbasis auch eine Lösung gehört, muss jeder Fall einen Handlungsvorschlag enthalten, der beschreibt, was in welcher Situation zu tun ist. \cite[vgl. S. 108f]{Weber2009} fügen jedem Fall hierzu eine Art Liste aus Aktionen hinzu, welche die KI vornehmen soll, um zu reagieren. Ein fertiger Fall könnte für dieses Beispiel ungefähr so aussehen wie Abbildung \ref{fig:Fall} darstellt. 

\begin{figure}[h]
	\centering
	\includegraphics[height=13.5cm]{Bilder/Fall.png}
	\caption{Fallbeispiel (\cite[vgl. S. 108]{Weber2009})}
	\label{fig:Fall}
\end{figure}
Wie die Abbildung \ref{fig:Fall} kann ein Fall schnell relativ umfangreich werden, wenn die Symptome nicht beschränkt sind. Wie zuvor beschrieben wurde auf weitere Symptome, den Gegner betreffend, verzichtet. Das ist nicht nur für die Zuverlässigkeit, sondern auch für die Aussagekraft selbst hilfreich. Selbst, wenn zwei Symptome zuverlässig erfasst werden können, kann es sein, dass die Erhebung eines dieser unterlassen werden kann, wenn sie im Grunde das Gleiche beschreiben. Eine solche Herangehensweise hilft dann, wenn die Fälle eben nicht zu umfangreich werden sollen. Für die Performance der KI kann dies sowohl Vorteile als auch Nachteile bedeuten: Einerseits kann ein solches System durch die Vermeidung ähnlicher Symptome die Übergewichtung dieser Bestandteile verhindern, da bei vielen korrelierenden Symptomen möglicherweise ein Fall gefunden wird, der wenig hilfreich ist. Andererseits werden dadurch auch Details vernachlässigt, die durchaus hilfreich für die Strategiefindung sein könnten. Insgesamt gilt es sehr genau zu betrachten, welche Eigenschaften des Spiels in einem Fall abzubilden sind und ob diese nötig sind bzw. einen Vorteil für die KI bringen. \\

Da in dem herangezogenen Paper \textit{Wargus} durch die KI gemeistert werden sollte und es sich hierbei um ein Echtzeitspiel handelt, sind die Ergebnisse möglicherweise nur teilweise auf rundenbasierte Spiele übertragbar. Bei Echtzeitspielen ändert sich die Situation, in der sich die KI wiederfindet, dauerhaft. Das ist für die Tätigung von Aktionen von besonderer Relevanz. Während die KI noch Anweisungen gibt, kann sich das Spiel schon soweit verändert haben, dass neue Aktionen nötig werden. Das erhöht den Schwierigkeitsgrad für die KI deutlich. Theoretisch kann kontinuierlich ein neuer Fall als Input entstehen, sodass die passende Strategie verändert werden müsste. Das erhöhe den Schwierigkeitsgrad der Konzeptionierung einer solchen KI, da zusätzlich festzulegen wäre, ob eine Strategie geändert, eingehalten oder gar abgebrochen werden soll, wenn sich die Situation im Spiel verändert. Es könnte auch eine Integration dieser Komponente in die Fälle stattfinden. Das erfolgt laut den Ausführungen von \cite[vgl. S. 109f]{Weber2009} für Ihre KI nicht, könnte aber Vorteile für Echtzeitstrategiespiele mit sich bringen.\\

Bei rundenbasierten Strategiespielen gibt es dieses Problem nicht in einem solchen Ausmaß. Da jede KI nur in einem bestimmten Zeitraum Aktionen tätigen darf, müssen auch nur innerhalb dieses Zeitraums Entscheidungen getroffen werden. Jedoch kann sich die Situation eines Spiels auch durch die eigenen Aktionen ändern und in dem betroffenen Zug sind die weiteren Entscheidungen dementsprechend zu überdenken. Dieses Problem könnte dadurch umgangen werden, dass ein Zug zu Beginn durchgeplant wird. Das bedeutet, dass über alle möglichen Tätigkeiten bereits vor der ersten eigenen Aktion schon entschieden wird, wobei das die Performanz beeinträchtigen könnte. Komplizierter wird das Problem jedoch, wenn Aktionen über mehrere Runden andauern. In einem solchen Fall müssten auch bei einem rundenbasierten Spiel zuvor getroffene bzw. anhaltende Entscheidungen, deren Umsetzung noch nicht abgeschlossen ist, in die Findung einer neuen Entscheidung miteinfließen. Gegenüber den Echtzeitspielen bleibt jedoch immer noch der Vorteil, dass keine dauerhafte Neubewertung der Situation erfolgen muss, da die Rundenbasiertheit vereinfachend auf das zeitliche Gefüge des Spielablaufs einwirkt - zumindest aus Sicht der KI-Spieler. \\

Die obigen Ausführungen über fallbasierte KIs werfen die Frage auf, ob es Alternativen gibt, die auf \textit{Case Based Reasoning} verzichten und stattdessen eine andere Grundlage für die Entscheidungsfindung verwenden. Theoretisch gibt es hierzu sogar gleich mehrere Möglichkeiten, denn das fallbasierte Lernen stellt nur eine mögliche Art des Lernens für Wissensbasierte Systeme dar. Weitere Arten des Lernen sind das inkrementelle, das analogiebasierte, das erklärungsbasierte sowie das Lernen durch Vergessen. Inwiefern diese sich für die Erstellung von KIs für Videospiele eigenen, bleibt allerdings fraglich und soll im Folgenden näher erläutert werden.\\

\textbf{Inkrementelles Lernen: }\\
Vom Namen dieses Lernverfahrens lässt sich bereits auf seine Eigenschaft schließen: Im Grunde werden mit jedem weiteren Lernschritt die Ergebnisse vorangegangener Schritte verbessert. Dieses Verfahren könnte in Verbindung mit dem fallbasierten Schließen eingesetzt werden, um dieses zu verbessern. So könnte durch den Einsatz inkrementellen Lernens die vorgeschlagene Lösung eines Falles variiert werden. Hierdurch würden bei jeder Anwendung bessere, schlechtere oder kaum veränderte Ergebnisse entstehen. Bei ersterem kann die Veränderung beibehalten oder sogar noch verstärkt, bei zweiterem negiert oder umgekehrt und bei letzterem müsste in eine beliebige Richtung verstärkt werden. Dies könnte dazu führen, dass die KI in ihrer Performanz teilweise stark schwankt, aber langfristig eine Verbesserung hinsichtlich ihrer Performanz festzustellen ist. Weiterhin ließe sich inkrementelles Lernen auch als alleiniges Verfahren einsetzen. Dazu müsste die Festhaltung des Wissen, aber in einer anderen Form als in Fällen erfolgen. Hier wären Regeln denkbar, die individuell bei eintretenden Ereignissen eine Antwort des Systems erzeugen und durch inkrementelles Lernen abgeändert werden können, um auch diese langfristig zu verbessern. Dies scheint auch eine Alternative zum fallbasierten Schließen darzustellen, da auch beim analogiebasierten Lernen eine individuellere Betrachtung der Ereignisse stattfinden kann. \cite[vgl. S. 207f]{Puppe1996}, \cite[vgl. V. 5 S. 35]{Althoff2019}

Im Grunde ist der Aufbau einer KI mithilfe eines Neuronales Netzes auch nichts anderes als die Anwendung inkrementellen Lernens. Bei Neuronalen Netzen wird durch Anpassung der Gewichte zwischen Neuronen nach jedem Lernschritt (Backpropagation) eine Veränderung des Verhaltens erreicht. Das wäre dann im eigentlichen Sinne kein WBS mehr, jedoch könnte ein Neuronales Netz die Wissensbasis ersetzen und über einen Interpreter, der wiederum wie bisher Informationen vom Spiel empfängt, angesteuert werden. Dieser wäre dann für die Vorbereitung der Eingabe für das Neuronale Netz und für die Generierung einer sinnhaften Anweisung aus der Ausgabe heraus zuständig.\\

\textbf{Analogiebasiertes Lernen: }\\
Beim analogiebasierten Lernen werden Schlüsse gezogen. Es wird versucht, Wissen von einem bereits bekannten Objekt auf ein neues Objekt zu übertragen und so die richtigen, analog angewendeten Schlüsse als gewonnenes Wissen explizit zu speichern (vgl. VL Althoff Wissenbasierte Systeme). Im Hinblick auf eine mögliche Übertragung auf Videospiele könnte hierbei die Ähnlichkeit von Situationen genutzt werden, um allgemeine Schlüsse ableiten zu können, die für all diese Arten von Situationen funktionieren. Dieser Ansatz scheint dem fallbasierten Schließen zu ähneln, da bei beiden mit der aktuellen Ähnlichkeit aktueller Situationen zur vorherigen gearbeitet wird. Der Unterschied besteht darin, dass beim analogiebasierten Lernen der Fokus auf dem expliziten Speichern dieses allgemeinen Wissens liegt. Der Vorteil hierin kann in der detaillierteren Anwendung bestehen. So muss nicht der ganze Fall genutzt werden, sondern es können ähnliche Aspekte der Situation genutzt werden, um einzelne Strategien zu übertragen und anzuwenden. Fallbasiertes Schließen hingegen würde den Vorteil der besseren Ausnahmebehandlung mit sich bringen, da so jede Ausnahmesituation in die Fallbasis Einzug findet und nicht auf eine andere Weise repräsentiert werden würde. \cite[vgl. V. 5 S. 12, 25, 28]{Althoff2019}\\

\textbf{Erklärungsbasiertes Lernen: }\\
Zur Anwendung erklärungsbasierten Lernens benötigt es drei Voraussetzungen: einen Zielbegriff, ein positives Beispiel und ein Operationalisierungskriterium, dem der Zielbegriff zu Anfang nicht genügt. Der Lerneffekt besteht darin, das Beispiel so zu generalisieren, dass es operational ist. Dieses Lernverfahren dann gut geeignet, wenn nachvollziehbares Wissen generiert werden soll in Form von Erklärungen. Zum Erstellen einer KI für ein rundenbasiertes Strategiespiel scheint diese Herangehensweise jedoch wenig geeignet zu sein: Die Konzeption einer solchen KI wäre möglich, allerdings scheint dieses Verfahren der KI keinen Vorteil zu bringen. Eine Art Implementierung dieser Variante wäre das WBS mit Beispielen von Siegen zu versorgen und es hieraus zu operationalisieren. \cite[vgl. V. 5 S. 38]{Althoff2019}\\

\textbf{Lernen durch Vergessen: }\\
Das Lernen durch Vergessen eignet sich zur Verbesserung von Systemen, indem Wissensinhalte abstrahiert und so vereinfacht dargestellt werden. Teilweise werden gespeicherte Informationen auch gelöscht. Auf diese Weise können Fehlinformationen oder zu starke Gewichtungen aufgelöst werden. Bezogen auf Strategiespiele könnte dieses Lernverfahren vielmehr als Ergänzung und nicht als Ersatz eines anderen fungieren. Angewandt aufs analogiebasiertes Lernen könnten einige Schlüsse gelöscht werden, wenn diese nicht vorteilig sind. Normalerweise könnte auch beim fallbasierten Schließen durch das Vergessen oder Abstrahieren von Regeln ein positiver Effekt erzeugt werden. Im Fall von \textit{Wargus} scheint dies jedoch nicht nötig zu sein, da der gesamte Fall bereits bekannt ist, und keine weiteren Symptomausprägungen erschlossen werden müssen. Je nach KI kann der Einsatz dieses Verfahrens durchaus in Erwägung gezogen werden. \cite[vgl. V. 5 S. 39]{Althoff2019}\\

Insgesamt lässt sich festhalten, dass es durchaus sinnvolle Alternativen zum fallbasierten Lernen für wissensbasierte KI-Spieler gibt. Einige sind dabei besser geeignet als andere und einige würden sogar eine gute Ergänzung darstellen. So eignet sich das erklärungsbasierte Lernen eher weniger für den Einsatz im beschriebenen Kontext und auch für den Anwendungsfall dieser Arbeit (rundenbasierte Strategiespiele) scheint dieses Lernverfahren keine wirklichen Vorteile zu bringen. Besser geeignet ist scheint hingegen das Lernen durch Vergessen. Dieses könnte für den hier beschriebenen Anwendungsfall die oben genannten Vorteile mit sich bringen, allerdings nur als Erweiterung in Kombination mit anderen Lernverfahren. Neben der eher geringen Einsetzbarkeit, in Verbindung mit den fallbasierten Schließen (\cite[vgl. S. 107f]{Weber2009}), können die Vorteile anderer Wissensrepräsentationsarten besser genutzt werden. So wäre der kombinierte Einsatz inkrementellen Lernens und des Lernens durch Vergessen möglicherweise eine gute Kombination, die an der später erläuterten Software gegen andere Lernverfahren getestet werden könnte. Das inkrementelle Lernen stellt bereits eine Alternative zum fallbasierten Lernen dar, wenn auch mit einer anderen Form der Wissensrepräsentation oder sogar als Neuronales Netz. Vor allem die individuelle Anwendbarkeit des Wissens scheint hier Möglichkeiten dazu zu bieten, die Fallbasiertheit nicht gewährleisten kann. Inwiefern sich die Behandlung von Ausnahmen negativ auf inkrementelles Lernen auswirkt, bleibt ohne weitere Tests nicht genau identifizierbar. Analogiebasiertes Lernen bildet eine weitere Alternative, die ebenso mit individuellen Antworten aufwarten kann. Hier ist die Form der Speicherung von Wissen ein interessanter Faktor, der Auswirkungen auf die Performanz einer späteren KI haben kann. 

Abschließend lässt sich zu den verschiedenen Lernstrategien festhalten, dass, obwohl \cite{Weber2009} nur die Strategie des fallbasierten Lernens angewandt wurde, einige weitere Verfahren zur Verfügung stehen und gegeneinander getestet werden können, um die bestmögliche KI zu entwickeln. In diesem Kapitel wurden zusätzlich verschiedene Vermutungen dazu angestellt, wie sich die unterschiedlichen Strategien auswirken und welche Strategie am Ende zu den besten Ergebnisse führt. Wirklich festzustellen ist dies aber nur durch ausgiebige Versuche, bei denen die KI-Spieler mit ihren unterschiedlichen Verfahren gegeneinander antreten.\\

\subsection{Der KI-Spieler als Neuronales Netz}
Eine Alternative zur als Wissensbasiertes System konstruierten KI bildet die Umsetzung durch ein Neuronales Netz. Diese wurde bereits im Abschnitt \ref{sec:WBS} kurz angesprochen, soll jedoch hier tiefgehender beschrieben werden. Abbildung \ref{fig:NN} zeigt wie ein Neuronales Netzwerk grundlegend aufgebaut ist. Im Detail zeigt die Abbildung ein Neuronales Netz, welches aus drei Schichten aufgebaut ist und insgesamt fünf Knoten beinhaltet: Zwei Inputknoten in der ersten Schicht, zwei Outputknoten im Hiddenlayer und ein Outputknoten in der Outputlayer. Es wird in dieser Darstellung (\ref{fig:NN}) auf die Verwendung des Bias verzichtet. \cite[vgl. S. 2ff]{Zupan1994} \\

\begin{figure}[h]
	\centering
	\includegraphics[height=4cm]{Bilder/NN.png}
	\caption{Beispiel neuronales Netz}
	\label{fig:NN}
\end{figure} 

Die Schichten sind über gewichtete Verbindungen mit der darauffolgenden Schicht verbunden, wobei jeder Knoten eine Verbindung mit jedem Knoten der nächsten Schicht aufweist. Wenn das Gewicht einer Verbindung auf null gesetzt wird, so wird diese nicht weiter beachtet und fungiert als würde sie nicht existieren. Der Wert eines Knotens in den folgenden Schichten wird auf Basis aller Knoten der vorherigen Schicht berechnet. So wird der Wert des Knotens (2,1) beispielsweise durch \textit{Wert((1,1)) * Gewicht der Verbindung((1,1),(2,1)) + Wert((1,2)) * Gewicht der Verbindung((1,2),(2,2))}. 

Durchgerechnet für das gesamte Beispiel ergibt sich daraus folgender Ablauf:\\
Wert((1,1)) = \textbf{1}\\
Wert((2,1)) = \textbf{2}\\
Wert((2,1)) = 1 * 0,5 + 2 * 0,5 = \textbf{1,5}\\
Wert((2,2)) = 1 * 0 + 2 * 2 = \textbf{4}\\
Wert((3,1)) = sig(1,5 * 2 + 4 * 0.25) = sig(4) = \textbf{~0,98} 

Der letzte Knoten stellt einen Sonderfall dar, weil hier eine Aktivierungsfunktion genutzt wird. Solche Funktionen dienen der weiteren Verarbeitung von Inputs. So sorgt die hier verwendete Sigmoidfunktion dafür, dass die Ausgabewerte zwischen 0 und 1 liegen. \\

Die Sigmoidfunktion stellt nicht die einzige mögliche Aktivierungsfunktion dar. Generell sind alle stetigen Funktionen einsetzbar, aber hier sollen nur einige, wichtige genannt werden:\\
\begin{enumerate}
	\item \textbf{Sigmoidfunktion} (\cite[vgl. S.5ff]{Nwankpa2018}): \scalebox{1.5}{$\frac{1}{1 + exp(-ax)}$}, mit a $\in \mathbb{R} $
	\item \textbf{Lineare Funktion} (\cite[vgl. S. 1ff]{Agostinelli2014}): \scalebox{1.2}{linear(x) = m*x + b}, mit m $\in \mathbb{R} $ der Steigung und b $\in \mathbb{R} $ dem Bias.
	\item \textbf{Stückweise lineare Funktion} (\cite[vgl. S. 1ff]{Agostinelli2014}): \\ 
	\scalebox{1.2}{l(x) = 
		$\begin{cases}
			1     & \text{ falls } x >= \frac{1}{2} \\
			a + \frac{1}{2} 	  & \text{ falls } -\frac{1}{2} < x < \frac{1}{2} \\
			0     & \text{ falls } x <= -\frac{1}{2}
		\end{cases}$}
	\item \textbf{ReLU} (\cite[vgl. S.8f]{Nwankpa2018}, \cite[vgl. S.3ff]{Ramachandran2017}): \scalebox{1.2}{ReLU(x) = max(0, x)}
	\item \textbf{Hard limit} (\cite[vgl. S.1040f]{Baraha2017}): \scalebox{1.2}{hard(x) = 
		$\begin{cases}
			1     & \text{ falls } x >= 0 \\
			0 	  & \text{ falls } x < 0 \\
		\end{cases}$}
\end{enumerate}

Die Entscheidung über den Einsatz der zu nutzenden Funktion ist u.a. vom Ziel abhängig, das mit dem Neuronalen Netz erreicht werden soll. Bezogen auf Videospiele wie \textit{Wargus} könnte beispielsweise die Outputlayer eines Netzes, welches über die Anzahl an zu erstellenden Truppen entscheidet, durch eine lineare Aktivierungsfunktion realisiert werden. Ob ein bestimmter taktischer Schritt ausgeführt wird, lässt sich dagegen eher durch eine Sigmoidfunktion abbilden (1 als ja und 0 als nein).\\

Bei dem Beispiel aus Abbildung \ref{fig:NN} handelt es sich nur um ein rudimentäres Neuronales Netz. Real eingesetzte Neuronale Netze sind deutlich umfangreicher. Sie unterscheiden sich in der Menge verwendeter Knoten pro Schicht sowie hinsichtlich der Anzahl der Schichten. So bestehen Neuronale Netze i.d.R. aus einer Input- und einer Outputlayer sowie beliebig viele hintereinanderliegenden Hiddenlayers. Außerdem können die Schichten abweichend vom Beispiel über eine beliebige Anzahl an Knoten verfügen (siehe Abb. \ref{fig:NN}), wobei nicht jede Schicht gleich viele Knoten haben muss. Wie viele Schichten und Knoten in einem Neuronalen Netz genutzt werden, hängt i.d.R. von der Komplexität der zu bewältigenden Aufgabe ab. Hierzu soll jedoch erwähnt werden, dass ein größeres Netz auch mehr Training benötigt, um die gewünschte Aufgabe zu erlernen. Dies hängt damit zusammen, dass mehr Verbindungen trainiert werden müssen. Einerseits stellt die Größe des Netzes einen Vorteil dar, andererseits kann dieser sich jedoch auch nachteilig auswirken, wenn nicht genügend Trainingsdaten verfügbar sind. \\

Das Trainieren eines Neuronalen Netzes ist ähnlich zum Finden von Koeffizienten einer Funktion deren Grad gleichzeitig bestimmt werden muss. Im Grunde ist ein Neuronales Netz nichts anderes als eine Funktion: Es erfolgt eine Eingabe und hierzu wird eine eindeutige Ausgabe geliefert. Der Vorteil  Neuronaler Netze, liegt darin begründet, dass sie ein einfaches Training mittels \textit{Backpropagation} ermöglichen. Bei \textit{Backpropagation} handelt es sich um ein Verfahren, dass einen errechneten Fehler zurück durch das Netz führt und damit anteilig jedes Gewicht anpasst, um den Fehler für die Zukunft zu minimieren \cite[vgl. S. 5f]{Zupan1994}. \\

Die Voraussetzung für Backpropagation ist es, dass zu einem Datensatz bereits das richtige Ergebnis bekannt ist. Dieser Datensatz wird als Input in das Neuronale Netz eingegeben und der Output mit dem tatsächlichen Ergebnis verglichen. Je nachdem mit wie vielen Datensätzen schon trainiert wurde, ist die Abweichung größer oder kleiner. Bei dieser Abweichung handelt es sich um den Fehler, also um die Differenz zwischen dem erwarteten und dem errechneten Ergebnis. Es muss nicht immer nur mit der Differenz gearbeitet werden. Abhängig vom Design des Neuronalen Netzes und von dem angewandten Lernverfahren kann nicht nur mit der Differenz, sondern auch mit dem quadrierten Fehler oder anderen Varianten trainiert werden. Anschließend an seine Berechnung wird der Fehler mittels des sogenannten \textit{Backwardpass} durch das Netz geführt und auf diese Weise auf die Gewichte der Verbindungen verteilt, um diese anzupassen. Entsprechend lässt sich festhalten, dass der Output beim \textit{Backwardpass} quasi als Input fungiert. \cite[vgl. S. 5ff]{Zupan1994}
 